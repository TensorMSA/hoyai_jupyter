{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Chatbot\n",
    "* 앱을 새로 깔 필요가 없음\n",
    "* 앱을 깔필요가 없으니 배울 것도 없음\n",
    "* 편한 UX - 그냥 텍스트 치면됨\n",
    "* 즉각적인 Feedback\n",
    "\n",
    "## Seq2Seq를 활용한 간단한 Q/A 봇을 만들어보자\n",
    "![이미지](http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png)\n",
    "* Python 3.5, Tensorflow 1.1, Konlpy (Mecab),Word2Vec (Gensim), matplotlib (Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq를 위한 Data 구성\n",
    "* 질의 응답별로 LIST로 구성\n",
    "* operator사용 value값 기준 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['피자', '주문', '할께']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sentence_length = 10\n",
    "dec_sentence_length = 10\n",
    "\n",
    "train_data = [\n",
    "    ['안녕', '만나서 반가워'],\n",
    "    ['넌누구니', '나는 AI 봇이란다.'],\n",
    "    ['피자 주문 할께', '페파로니 주문해줘'],\n",
    "    ['음료는 멀로', '콜라로 해줘']\n",
    "]\n",
    "\n",
    "\n",
    "all_input_sentences = []\n",
    "all_target_sentences = []\n",
    "\n",
    "for row_data in train_data:\n",
    "    all_input_sentences.append(row_data[0])\n",
    "    all_target_sentences.append(row_data[1])\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    tokens = mecab.morphs(sentence)\n",
    "    return tokens\n",
    "\n",
    "tokenizer('피자 주문 할께')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector 구성 (입력된 문장의 글자별 Vector)\n",
    " - 일반적으로 처리단위가 작아질수록 미등록어에서 자유롭고 작은 vector 차원을 유지할 수 있지만\n",
    " - 문장의 길이가 길어지고, 학습이 어려워지는 문제가 있기에 적절한 embedding을 찾아야하는데 \n",
    " - 이부분은 Biz Domain 별 차이가 있음 복잡도나 표현 가능성등을 적절한 균형에서 찾아야함 \n",
    " - 아래 소스는 이해하기 쉽도록 글자단위의 Onehot으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocabulary size: 11\n",
      "target vocabulary size: 17\n",
      "피자 8\n",
      "주문 2\n",
      "할께 9\n",
      "피자 주문 할께\n",
      "([8, 2, 9, 0, 0, 0, 0, 0, 0, 0], 3)\n",
      "페파로니 주문해줘\n",
      "([0, 10, 13, 7, 3, 2, 1, 1, 1, 1, 1], 5)\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences, is_target=False, max_vocab_size=None):\n",
    "    word_counter = Counter()\n",
    "    vocab = dict()\n",
    "    reverse_vocab = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        word_counter.update(tokens)\n",
    "\n",
    "    if max_vocab_size is None:\n",
    "        max_vocab_size = len(word_counter)\n",
    "\n",
    "    if is_target:\n",
    "        vocab['_GO'] = 0\n",
    "        vocab['_PAD'] = 1\n",
    "        vocab_idx = 2\n",
    "        for key, value in word_counter.most_common(max_vocab_size):\n",
    "            vocab[key] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "    else:\n",
    "        vocab['_PAD'] = 0\n",
    "        vocab_idx = 1\n",
    "        for key, value in word_counter.most_common(max_vocab_size):\n",
    "            vocab[key] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "\n",
    "    for key, value in vocab.items():\n",
    "        reverse_vocab[value] = key\n",
    "\n",
    "    return vocab, reverse_vocab, max_vocab_size\n",
    "\n",
    "enc_vocab, enc_reverse_vocab, enc_vocab_size = build_vocab(all_input_sentences)\n",
    "dec_vocab, dec_reverse_vocab, dec_vocab_size = build_vocab(all_target_sentences, is_target=True)\n",
    "\n",
    "print('input vocabulary size:', enc_vocab_size)\n",
    "print('target vocabulary size:', dec_vocab_size)\n",
    "\n",
    "def token2idx(word, vocab):\n",
    "    return vocab[word]\n",
    "\n",
    "for token in tokenizer('피자 주문 할께'):\n",
    "    print(token, token2idx(token, enc_vocab))\n",
    "\n",
    "def sent2idx(sent, vocab=enc_vocab, max_sentence_length=enc_sentence_length, is_target=False):\n",
    "    tokens = tokenizer(sent)\n",
    "    current_length = len(tokens)\n",
    "    pad_length = max_sentence_length - current_length\n",
    "    if is_target:\n",
    "        return [0] + [token2idx(token, vocab) for token in tokens] + [1] * pad_length, current_length\n",
    "    else:\n",
    "        return [token2idx(token, vocab) for token in tokens] + [0] * pad_length, current_length\n",
    "\n",
    "print('피자 주문 할께')\n",
    "print(sent2idx('피자 주문 할께'))\n",
    "\n",
    "print('페파로니 주문해줘')\n",
    "print(sent2idx('페파로니 주문해줘', vocab=dec_vocab, max_sentence_length=dec_sentence_length, is_target=True))\n",
    "\n",
    "def idx2token(idx, reverse_vocab):\n",
    "    return reverse_vocab[idx]\n",
    "\n",
    "def idx2sent(indices, reverse_vocab=dec_reverse_vocab):\n",
    "    return \" \".join([idx2token(idx, reverse_vocab) for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel():\n",
    "    def __init__(self, mode='training'):\n",
    "        self.mode = mode\n",
    "\n",
    "        self.hidden_size = 30\n",
    "        self.enc_emb_size = 30\n",
    "        self.dec_emb_size = 30\n",
    "        self.attn_size = 30\n",
    "        self.cell = tf.contrib.rnn.BasicLSTMCell\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer\n",
    "        self.n_epoch = 101\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.start_token = 0\n",
    "        self.end_token = 1\n",
    "\n",
    "        self.ckpt_dir = './ckpt_dir/'\n",
    "\n",
    "    def add_decoder(self):\n",
    "        with tf.variable_scope('Decoder'):\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.dec_Wemb = tf.get_variable('embedding',\n",
    "                                                initializer=tf.random_uniform([dec_vocab_size + 2, self.dec_emb_size]),\n",
    "                                                dtype=tf.float32)\n",
    "\n",
    "            batch_size = tf.shape(self.enc_inputs)[0]\n",
    "\n",
    "            dec_cell = self.cell(self.hidden_size)\n",
    "\n",
    "            attn_mech = tf.contrib.seq2seq.LuongAttention(\n",
    "                num_units=self.attn_size,\n",
    "                memory=self.enc_outputs,\n",
    "                memory_sequence_length=self.enc_sequence_length,\n",
    "                name='LuongAttention')\n",
    "\n",
    "            dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell=dec_cell,\n",
    "                attention_mechanism=attn_mech,\n",
    "                attention_layer_size=self.attn_size,\n",
    "                name='Attention_Wrapper')\n",
    "\n",
    "            initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "            output_layer = Dense(dec_vocab_size + 2, name='output_projection')\n",
    "\n",
    "            if self.mode == 'training':\n",
    "                max_dec_len = tf.reduce_max(self.dec_sequence_length + 1, name='max_dec_len')\n",
    "\n",
    "                dec_emb_inputs = tf.nn.embedding_lookup(\n",
    "                    self.dec_Wemb, self.dec_inputs, name='emb_inputs')\n",
    "\n",
    "                training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                    inputs=dec_emb_inputs,\n",
    "                    sequence_length=self.dec_sequence_length + 1,\n",
    "                    time_major=False,\n",
    "                    name='training_helper')\n",
    "\n",
    "                training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=training_helper,\n",
    "                    initial_state=initial_state,\n",
    "                    output_layer=output_layer)\n",
    "\n",
    "                train_dec_outputs, train_dec_last_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    training_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=max_dec_len)\n",
    "\n",
    "                logits = tf.identity(train_dec_outputs.rnn_output, name='logits')\n",
    "                targets = tf.slice(self.dec_inputs, [0, 0], [-1, max_dec_len], 'targets')\n",
    "                masks = tf.sequence_mask(self.dec_sequence_length + 1, max_dec_len, dtype=tf.float32, name='masks')\n",
    "\n",
    "                self.batch_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits=logits,\n",
    "                    targets=targets,\n",
    "                    weights=masks,\n",
    "                    name='batch_loss')\n",
    "\n",
    "                self.valid_predictions = tf.identity(train_dec_outputs.sample_id, name='valid_preds')\n",
    "\n",
    "            elif self.mode == 'inference':\n",
    "\n",
    "                start_tokens = tf.tile(tf.constant([self.start_token], dtype=tf.int32), [batch_size],\n",
    "                                       name='start_tokens')\n",
    "\n",
    "                inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding=self.dec_Wemb,\n",
    "                    start_tokens=start_tokens,\n",
    "                    end_token=self.end_token)\n",
    "\n",
    "                inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=inference_helper,\n",
    "                    initial_state=initial_state,\n",
    "                    output_layer=output_layer)\n",
    "\n",
    "                infer_dec_outputs, infer_dec_last_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    inference_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=dec_sentence_length)\n",
    "\n",
    "                self.predictions = tf.identity(infer_dec_outputs.sample_id, name='predictions')\n",
    "\n",
    "    def save(self, sess, var_list=None, save_path=None):\n",
    "        print('Saving model at {save_path}')\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.save(sess, save_path, write_meta_graph=False)\n",
    "\n",
    "    def build(self):\n",
    "        self.enc_inputs = tf.placeholder(tf.int32, shape=[None, enc_sentence_length], name='input_sentences')\n",
    "        self.enc_sequence_length = tf.placeholder(tf.int32, shape=[None, ], name='input_sequence_length')\n",
    "\n",
    "        if self.mode == 'training':\n",
    "            self.dec_inputs = tf.placeholder(tf.int32, shape=[None, dec_sentence_length + 1], name='target_sentences')\n",
    "            self.dec_sequence_length = tf.placeholder(tf.int32, shape=[None, ], name='target_sequence_length')\n",
    "\n",
    "        with tf.variable_scope('Encoder'):\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.enc_Wemb = tf.get_variable('embedding',\n",
    "                                                initializer=tf.random_uniform([enc_vocab_size + 1, self.enc_emb_size]),\n",
    "                                                dtype=tf.float32)\n",
    "\n",
    "            enc_emb_inputs = tf.nn.embedding_lookup(self.enc_Wemb, self.enc_inputs, name='emb_inputs')\n",
    "            enc_cell = self.cell(self.hidden_size)\n",
    "\n",
    "            self.enc_outputs, self.enc_last_state = tf.nn.dynamic_rnn(\n",
    "                cell=enc_cell,\n",
    "                inputs=enc_emb_inputs,\n",
    "                sequence_length=self.enc_sequence_length,\n",
    "                time_major=False,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "        self.add_decoder()\n",
    "\n",
    "    def train(self, sess, data, save_path=None):\n",
    "        print(data)\n",
    "\n",
    "        self.training_op = self.optimizer(self.learning_rate, name='training_op').minimize(self.batch_loss)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in tqdm(range(self.n_epoch)):\n",
    "\n",
    "            all_preds = []\n",
    "            epoch_loss = 0\n",
    "            for row_data in data:\n",
    "                input_batch_tokens = []\n",
    "                target_batch_tokens = []\n",
    "                enc_sentence_lengths = []\n",
    "                dec_sentence_lengths = []\n",
    "\n",
    "                tokens, sent_len = sent2idx(row_data[0])\n",
    "                input_batch_tokens.append(tokens)\n",
    "                enc_sentence_lengths.append(sent_len)\n",
    "\n",
    "                tokens, sent_len = sent2idx(row_data[1],\n",
    "                                            vocab=dec_vocab,\n",
    "                                            max_sentence_length=dec_sentence_length,\n",
    "                                            is_target=True)\n",
    "                target_batch_tokens.append(tokens)\n",
    "                dec_sentence_lengths.append(sent_len)\n",
    "\n",
    "                batch_preds, batch_loss, _ = sess.run(\n",
    "                    [self.valid_predictions, self.batch_loss, self.training_op],\n",
    "                    feed_dict={\n",
    "                        self.enc_inputs: input_batch_tokens,\n",
    "                        self.enc_sequence_length: enc_sentence_lengths,\n",
    "                        self.dec_inputs: target_batch_tokens,\n",
    "                        self.dec_sequence_length: dec_sentence_lengths,\n",
    "                    })\n",
    "                epoch_loss += batch_loss\n",
    "                all_preds.append(batch_preds)\n",
    "\n",
    "            loss_history.append(epoch_loss)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('Epoch', epoch)\n",
    "                for row_data, batch_preds in zip(data, all_preds):\n",
    "\n",
    "                    for input_sent, target_sent, pred in zip(row_data[0], row_data[1], batch_preds):\n",
    "                        print('\\tInput: {input_sent}')\n",
    "                        print('\\tPrediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab))\n",
    "                        print('\\tTarget:, {target_sent}')\n",
    "                print('\\tepoch loss: {epoch_loss:.2f}\\n')\n",
    "\n",
    "        if save_path:\n",
    "            self.save(sess, save_path=save_path)\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def inference(self, sess, data, load_ckpt):\n",
    "        self.restorer = tf.train.Saver().restore(sess, load_ckpt)\n",
    "\n",
    "        batch_tokens = []\n",
    "        batch_sent_lens = []\n",
    "\n",
    "        tokens, sent_len = sent2idx(data)\n",
    "        batch_tokens.append(tokens)\n",
    "        batch_sent_lens.append(sent_len)\n",
    "\n",
    "        batch_preds = sess.run(\n",
    "            self.predictions,\n",
    "            feed_dict={\n",
    "                self.enc_inputs: batch_tokens,\n",
    "                self.enc_sequence_length: batch_sent_lens,\n",
    "            })\n",
    "\n",
    "        print('Input:', data)\n",
    "        print('Prediction:', idx2sent(batch_preds[0], reverse_vocab=dec_reverse_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장을 위한 함수\n",
    "* 현재폴더의 model폴더를 만들어 모델을 저장한다 \n",
    "* 모델이 존재할 경우 삭제하고 새로 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Graph 생성\n",
    "* seq2seq모델의 Graph 생성\n",
    "* 동일한 크기의 encoder과 decoder의 크기로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습결과 출력\n",
    "* matplotlib 활용 학습 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = Seq2SeqModel(mode='training')\n",
    "    model.build()\n",
    "    loss_history = model.train(sess, train_data, save_path=model.ckpt_dir+'epoch_{model.n_epoch}_attention')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(range(model.n_epoch), loss_history, label = 'cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = Seq2SeqModel(mode='inference')\n",
    "    model.build()\n",
    "    for row_data in train_data:\n",
    "        model.inference(sess, row_data[0], load_ckpt=model.ckpt_dir+'epoch_{model.n_epoch}_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}